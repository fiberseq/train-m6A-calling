{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3cf36b-9900-400c-827d-4293ff4dba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "m6a_cnn.py\n",
    "Author: Anupama Jha <anupamaj@uw.edu>\n",
    "This module predicts whether an\n",
    "adenine is methylated or not. The\n",
    "model is trained with Fiber-seq\n",
    "HiFi read sequence, inter-pulse\n",
    "distance and pulse width signal\n",
    "from pacbio. The model is a\n",
    "convolution neural network.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n",
    "class M6ANet(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, input_size=6, sec_last_layer_size=25, last_layer_size=5, output_shape=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for the M6ANet, a CNN\n",
    "        model for m6A calling.\n",
    "        :param input_size: int, number of\n",
    "                                channels in\n",
    "                                the data,\n",
    "                                currently 6,\n",
    "                                4 for nucleotide\n",
    "                                identity, one for\n",
    "                                inter-pulse distance\n",
    "                                and one for pulse width.\n",
    "        :param sec_last_layer_size: int, size of the second\n",
    "                                         last dense layer.\n",
    "        :param last_layer_size: int, size of the last dense\n",
    "                                     layer.\n",
    "        :param output_shape: int, number of outputs, two in\n",
    "                                  our case, m6A or not.\n",
    "        \"\"\"\n",
    "        super(M6ANet, self).__init__()\n",
    "\n",
    "        # Three convolution layers with ReLU activation\n",
    "        self.conv_1 = torch.nn.Conv1d(\n",
    "            in_channels=input_size, out_channels=30, kernel_size=5, stride=1\n",
    "        )\n",
    "\n",
    "        self.relu_1 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv1d(\n",
    "            in_channels=30, out_channels=10, kernel_size=5, stride=1\n",
    "        )\n",
    "\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv1d(\n",
    "            in_channels=10, out_channels=5, kernel_size=3, stride=1\n",
    "        )\n",
    "\n",
    "        self.relu_3 = torch.nn.ReLU()\n",
    "\n",
    "        # a dense layer with ReLU activation\n",
    "        self.linear = torch.nn.Linear(\n",
    "            in_features=sec_last_layer_size, out_features=last_layer_size\n",
    "        )\n",
    "\n",
    "        self.relu_4 = torch.nn.ReLU()\n",
    "\n",
    "        # an output dense layer with no activation\n",
    "        self.label = torch.nn.Linear(\n",
    "            in_features=last_layer_size, out_features=output_shape\n",
    "        )\n",
    "\n",
    "        # Loss function\n",
    "        self.cross_entropy_loss = torch.nn.BCELoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward function to go\n",
    "        from input to output\n",
    "        of the model.\n",
    "        :param X: Tensor, input to\n",
    "                          the model.\n",
    "        :return: y: tensor, output\n",
    "                            from the\n",
    "                            model.\n",
    "        \"\"\"\n",
    "        # Three convolutional layers\n",
    "        # with ReLU activation\n",
    "        X = self.relu_1(self.conv_1(X))\n",
    "        X = self.relu_2(self.conv_2(X))\n",
    "        X = self.relu_3(self.conv_3(X))\n",
    "\n",
    "        # Condense 2D shape to 1D\n",
    "        X = torch.flatten(X, 1)\n",
    "\n",
    "        # Dense layer with ReLU activation\n",
    "        X = self.relu_4(self.linear(X))\n",
    "\n",
    "        # Output layer\n",
    "        y = torch.nn.Softmax(dim=1)(self.label(X))\n",
    "        return y\n",
    "\n",
    "    def predict(self, X, batch_size=64, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Predict function to generate\n",
    "        M6ANet model predictions.\n",
    "        :param X: tensor, input data\n",
    "        :param batch_size: int, batch\n",
    "                                size for\n",
    "                                generating\n",
    "                                predictions in.\n",
    "        :param device: str, cpu or cuda\n",
    "        :return: m6a_labels: tensor, predictions\n",
    "        \"\"\"\n",
    "        # Turn off gradient\n",
    "        # computation\n",
    "        with torch.no_grad():\n",
    "            # set model to\n",
    "            # evaluation mode\n",
    "            self.eval()\n",
    "\n",
    "            # Get batch start indices\n",
    "            starts = np.arange(0, X.shape[0], batch_size)\n",
    "\n",
    "            # Get batch end indices\n",
    "            ends = starts + batch_size\n",
    "\n",
    "            # m6a labels\n",
    "            m6a_labels = []\n",
    "\n",
    "            # Get predictions for every batch\n",
    "            for start, end in zip(starts, ends):\n",
    "                X_batch = X[start:end].to(device)\n",
    "\n",
    "                # Run the data through the forward\n",
    "                # function to generate label predictions\n",
    "                m6a_labels_batch = self(X_batch)\n",
    "\n",
    "                # Move the label predictions to the CPU\n",
    "                m6a_labels_batch = m6a_labels_batch.cpu()\n",
    "\n",
    "                # Append to the list of all labels\n",
    "                m6a_labels.append(m6a_labels_batch)\n",
    "\n",
    "            # Make one list of all labels\n",
    "            m6a_labels = torch.cat(m6a_labels)\n",
    "            return m6a_labels\n",
    "\n",
    "    def evaluate(self,\n",
    "                 X_valid,\n",
    "                 y_valid,\n",
    "                 device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Generate predictions for validation\n",
    "        data and compute average precision.\n",
    "        :param X_valid: np.array, validation\n",
    "                                  features\n",
    "        :param y_valid: np.array, validation\n",
    "                                  labels\n",
    "        :param device: str, cpu or cuda\n",
    "        :return: float, average precision\n",
    "        \"\"\"\n",
    "        # Convert validation data into tensors\n",
    "        X_valid = torch.tensor(X_valid).float()\n",
    "        y_valid = torch.tensor(y_valid).float()\n",
    "        X_valid = X_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Set the model to\n",
    "            # evaluation mode\n",
    "            self.eval()\n",
    "\n",
    "            # Compute the predictions for\n",
    "            # the validation set\n",
    "            valid_preds = self.predict(X_valid, device=device)\n",
    "\n",
    "            # Compute AUPR/Average precision\n",
    "            sklearn_ap = average_precision_score(\n",
    "                y_valid.cpu().numpy()[:, 0], valid_preds.cpu().numpy()[:, 0]\n",
    "            )\n",
    "\n",
    "        return sklearn_ap\n",
    "\n",
    "    def fit_semisupervised(\n",
    "            self,\n",
    "            training_data,\n",
    "            model_optimizer,\n",
    "            X_valid=None,\n",
    "            y_valid=None,\n",
    "            max_epochs=10,\n",
    "            validation_iter=1000,\n",
    "            device=\"cpu\",\n",
    "            best_save_model=\"\",\n",
    "            final_save_model=\"\",\n",
    "            prev_aupr=0,\n",
    "            input_example=(1, 6, 15)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training procedure for the semi-supervised version\n",
    "        of m6A CNN.\n",
    "        :param training_data: torch.DataLoader,\n",
    "                              training data generator\n",
    "        :param model_optimizer: torch.Optimizer,\n",
    "                                An optimizer to\n",
    "                                training our model\n",
    "        :param X_valid: numpy array, validation features\n",
    "        :param y_valid: numpy array, validation labels\n",
    "        :param max_epochs: int, maximum epochs to run\n",
    "                                the model for\n",
    "        :param validation_iter: int,After how many\n",
    "                                    iterations should\n",
    "                                    we compute validation\n",
    "                                    stats.\n",
    "        :param device: str, GPU versus CPU, defaults to CPU\n",
    "        :param best_save_model: str, path to save best model\n",
    "        :param final_save_model: str, path to save final model\n",
    "        :param prev_aupr: float, best precision so far,\n",
    "                                 relevant for semi-supervised\n",
    "                                 training\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Convert validation data into tensors\n",
    "        X_valid = torch.tensor(X_valid).float()\n",
    "        y_valid = torch.tensor(y_valid).float()\n",
    "        X_valid = X_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "\n",
    "        best_aupr = prev_aupr\n",
    "        for epoch in range(max_epochs):\n",
    "            # to log cross-entropy loss to\n",
    "            # average over batches\n",
    "            avg_train_loss = 0\n",
    "            avg_train_iter = 0\n",
    "            iteration = 0\n",
    "            for data in training_data:\n",
    "                # Get features and label batch\n",
    "                X, y = data\n",
    "                # Convert them to float\n",
    "                X = X.float()\n",
    "                y = y.float()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # Clear the optimizer and\n",
    "                # set the model to training mode\n",
    "                model_optimizer.zero_grad()\n",
    "                self.train()\n",
    "\n",
    "                # Run forward pass\n",
    "                m6a_labels = self.forward(X)\n",
    "\n",
    "                # Calculate the cross entropy loss\n",
    "                cross_entropy_loss = self.cross_entropy_loss(m6a_labels, y)\n",
    "\n",
    "                # Extract the cross entropy loss for logging\n",
    "                cross_entropy_loss_item = cross_entropy_loss.item()\n",
    "\n",
    "                # Do the back propagation\n",
    "                cross_entropy_loss.backward()\n",
    "                model_optimizer.step()\n",
    "\n",
    "                # log loss to average over training batches\n",
    "                avg_train_loss += cross_entropy_loss_item\n",
    "                avg_train_iter += 1\n",
    "\n",
    "                # If current iteration is a\n",
    "                # validation iteration\n",
    "                # compute validation stats.\n",
    "                if iteration > 0 and iteration % validation_iter == 0:\n",
    "                    with torch.no_grad():\n",
    "                        # Set the model to\n",
    "                        # evaluation mode\n",
    "                        self.eval()\n",
    "\n",
    "                        # Convert one hot encoded labels\n",
    "                        # to number based labels\n",
    "                        # ([0, 1] -> 1, [1, 0] -> 0)\n",
    "                        y_valid_metric = torch.argmax(y_valid, dim=1).int()\n",
    "\n",
    "                        # Compute the predictions for\n",
    "                        # the validation set\n",
    "                        valid_preds = self.predict(X_valid, device=device)\n",
    "                        # Move predictions to CPU/GPU\n",
    "                        valid_preds = valid_preds.to(device)\n",
    "\n",
    "                        # Compute AUPR/Average precision\n",
    "                        sklearn_ap = average_precision_score(\n",
    "                            y_valid.cpu().numpy()[:, 0], valid_preds.cpu().numpy()[:, 0]\n",
    "                        )\n",
    "                        if verbose:\n",
    "                            # Convert one hot encoded predictions\n",
    "                            # to number based labels\n",
    "                            # ([0, 1] -> 1, [1, 0] -> 0)\n",
    "                            pred_valid_metric = torch.argmax(valid_preds, dim=1).int()\n",
    "\n",
    "                            # compute cross_entropy loss\n",
    "                            # for the validation set.\n",
    "                            cross_entropy_loss = self.cross_entropy_loss(\n",
    "                                valid_preds, y_valid\n",
    "                            )\n",
    "\n",
    "                            # Extract the validation loss\n",
    "                            valid_loss = cross_entropy_loss.item()\n",
    "                            # Compute AUROC\n",
    "                            sklearn_rocauc = roc_auc_score(\n",
    "                                y_valid.cpu().numpy()[:, 0],\n",
    "                                valid_preds.cpu().numpy()[:, 0],\n",
    "                            )\n",
    "                            # Compute accuracy\n",
    "                            sklearn_acc = accuracy_score(\n",
    "                                y_valid_metric.cpu().numpy(),\n",
    "                                pred_valid_metric.cpu().numpy(),\n",
    "                            )\n",
    "                            train_loss = avg_train_loss / avg_train_iter\n",
    "                            print(\n",
    "                                f\"Epoch {epoch}, iteration {iteration},\"\n",
    "                                f\" train loss: {train_loss:4.4f},\"\n",
    "                                f\" validation loss: {valid_loss:4.4f}\"\n",
    "                            )\n",
    "\n",
    "                            print(\n",
    "                                f\"Validation iteration {iteration}, \"\n",
    "                                f\"AUPR: {sklearn_ap},\"\n",
    "                                f\" Accuracy: {sklearn_acc}, \"\n",
    "                                f\"AUROC: {sklearn_rocauc}\"\n",
    "                            )\n",
    "\n",
    "                        if sklearn_ap > best_aupr:\n",
    "                            with open(best_save_model, \"wb\") as fp:\n",
    "                                pickle.dump(self.state_dict(), fp)\n",
    "                            best_aupr = sklearn_ap\n",
    "                            # save rust model\n",
    "                            example = torch.rand(input_example).float().to(device)\n",
    "                            traced_script_module = torch.jit.trace(self, example)\n",
    "                            traced_script_module.save(f\"{best_save_model}.pt\")\n",
    "\n",
    "                        avg_train_loss = 0\n",
    "                        avg_train_iter = 0\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "        with open(final_save_model, \"wb\") as fp:\n",
    "            pickle.dump(self.state_dict(), fp)\n",
    "        # save rust model\n",
    "        example = torch.rand(input_example).float().to(device)\n",
    "        traced_script_module = torch.jit.trace(self, example)\n",
    "        traced_script_module.save(f\"{final_save_model}.pt\")\n",
    "        \n",
    "\n",
    "    def fit_supervised(\n",
    "            self,\n",
    "            training_data,\n",
    "            model_optimizer,\n",
    "            X_valid=None,\n",
    "            y_valid=None,\n",
    "            max_epochs=10,\n",
    "            validation_iter=1000,\n",
    "            device=\"cpu\",\n",
    "            best_save_model=\"\",\n",
    "            final_save_model=\"\",\n",
    "            input_example=(1, 6, 15)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training procedure for the supervised version\n",
    "        of m6A CNN.\n",
    "        :param training_data: torch.DataLoader,\n",
    "                              training data generator\n",
    "        :param model_optimizer: torch.Optimizer,\n",
    "                                An optimizer to\n",
    "                                training our model\n",
    "        :param X_valid: numpy array, validation features\n",
    "        :param y_valid: numpy array, validation labels\n",
    "        :param max_epochs: int, maximum epochs to run\n",
    "                                the model for\n",
    "        :param validation_iter: int,After how many\n",
    "                                    iterations should\n",
    "                                    we compute validation\n",
    "                                    stats.\n",
    "        :param device: str, GPU versus CPU, defaults to CPU\n",
    "        :param best_save_model: str, path to save best model\n",
    "        :param final_save_model: str, path to save final model\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Convert validation data into tensors\n",
    "        X_valid = torch.tensor(X_valid).float()\n",
    "        y_valid = torch.tensor(y_valid).float()\n",
    "        X_valid = X_valid.to(device)\n",
    "        y_valid = y_valid.to(device)\n",
    "\n",
    "        best_aupr = 0\n",
    "        for epoch in range(max_epochs):\n",
    "            # to log cross-entropy loss to\n",
    "            # average over batches\n",
    "            avg_train_loss = 0\n",
    "            avg_train_iter = 0\n",
    "            iteration = 0\n",
    "            for data in training_data:\n",
    "                # Get features and label batch\n",
    "                X, y = data\n",
    "                # Convert them to float\n",
    "                X = X.float()\n",
    "                y = y.float()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # Clear the optimizer and\n",
    "                # set the model to training mode\n",
    "                model_optimizer.zero_grad()\n",
    "                self.train()\n",
    "\n",
    "                # Run forward pass\n",
    "                m6a_labels = self.forward(X)\n",
    "\n",
    "                # Calculate the cross entropy loss\n",
    "                cross_entropy_loss = self.cross_entropy_loss(m6a_labels, y)\n",
    "\n",
    "                # Extract the cross entropy loss for logging\n",
    "                cross_entropy_loss_item = cross_entropy_loss.item()\n",
    "\n",
    "                # Do the back propagation\n",
    "                cross_entropy_loss.backward()\n",
    "                model_optimizer.step()\n",
    "\n",
    "                # log loss to average over training batches\n",
    "                avg_train_loss += cross_entropy_loss_item\n",
    "                avg_train_iter += 1\n",
    "\n",
    "                # If current iteration is a\n",
    "                # validation iteration\n",
    "                # compute validation stats.\n",
    "                if iteration % validation_iter == 0:\n",
    "                    with torch.no_grad():\n",
    "                        # Set the model to\n",
    "                        # evaluation mode\n",
    "                        self.eval()\n",
    "\n",
    "                        # Convert one hot encoded labels\n",
    "                        # to number based labels\n",
    "                        # ([0, 1] -> 1, [1, 0] -> 0)\n",
    "                        y_valid_metric = torch.argmax(y_valid, dim=1).int()\n",
    "\n",
    "                        # Compute the predictions for\n",
    "                        # the validation set\n",
    "                        valid_preds = self.predict(X_valid, device=device)\n",
    "                        # Move predictions to CPU/GPU\n",
    "                        valid_preds = valid_preds.to(device)\n",
    "\n",
    "                        # Convert one hot encoded predictions\n",
    "                        # to number based labels\n",
    "                        # ([0, 1] -> 1, [1, 0] -> 0)\n",
    "                        pred_valid_metric = torch.argmax(valid_preds, dim=1).int()\n",
    "\n",
    "                        # compute cross_entropy loss\n",
    "                        # for the validation set.\n",
    "                        cross_entropy_loss = self.cross_entropy_loss(\n",
    "                            valid_preds, y_valid\n",
    "                        )\n",
    "\n",
    "                        # Extract the validation loss\n",
    "                        valid_loss = cross_entropy_loss.item()\n",
    "\n",
    "                        # Compute AUROC\n",
    "                        sklearn_rocauc = roc_auc_score(\n",
    "                            y_valid.cpu().numpy()[:, 0], valid_preds.cpu().numpy()[:, 0]\n",
    "                        )\n",
    "\n",
    "                        # Compute AUPR/Average precision\n",
    "                        sklearn_ap = average_precision_score(\n",
    "                            y_valid.cpu().numpy()[:, 0], valid_preds.cpu().numpy()[:, 0]\n",
    "                        )\n",
    "\n",
    "                        # Compute accuracy\n",
    "                        sklearn_acc = accuracy_score(\n",
    "                            y_valid_metric.cpu().numpy(),\n",
    "                            pred_valid_metric.cpu().numpy(),\n",
    "                        )\n",
    "                        train_loss = avg_train_loss / avg_train_iter\n",
    "\n",
    "                        print(\n",
    "                            f\"Epoch {epoch}, iteration {iteration},\"\n",
    "                            f\" train loss: {train_loss:4.4f},\"\n",
    "                            f\" validation loss: {valid_loss:4.4f}\"\n",
    "                        )\n",
    "\n",
    "                        print(\n",
    "                            f\"Validation iteration {iteration}, \"\n",
    "                            f\"AUPR: {sklearn_ap},\"\n",
    "                            f\" Accuracy: {sklearn_acc}, \"\n",
    "                            f\"AUROC: {sklearn_rocauc}\"\n",
    "                        )\n",
    "\n",
    "                        if sklearn_ap > best_aupr:\n",
    "                            with open(best_save_model, \"wb\") as fp:\n",
    "                                pickle.dump(self.state_dict(), fp)\n",
    "                            best_aupr = sklearn_ap\n",
    "                            # save rust model\n",
    "                            example = torch.rand(input_example).float().to(device)\n",
    "                            traced_script_module = torch.jit.trace(self, example)\n",
    "                            traced_script_module.save(f\"{best_save_model}.pt\")\n",
    "\n",
    "                        avg_train_loss = 0\n",
    "                        avg_train_iter = 0\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "        with open(final_save_model, \"wb\") as fp:\n",
    "            pickle.dump(self.state_dict(), fp)\n",
    "        \n",
    "        # save rust model\n",
    "        example = torch.rand(input_example).float().to(device)\n",
    "        traced_script_module = torch.jit.trace(self, example)\n",
    "        traced_script_module.save(f\"{final_save_model}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c69e1c2-a3b1-492d-9e9f-38b9e5a52f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sup_save_model = \"paper_v1/models/m6A_3_2_semi_supervised_cnn.best.torch.pickle\"\n",
    "\n",
    "input_example = (1, 6, 15)\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the supervised model for transfer learning\n",
    "model = M6ANet()\n",
    "with open(best_sup_save_model, \"rb\") as fp:\n",
    "    model.load_state_dict(pickle.load(fp))\n",
    "        \n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "example = torch.rand(input_example).float().to(device)\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "traced_script_module.save(f\"{best_sup_save_model}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6648f10c-4711-4457-b5e6-1e037c0485ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%actual_input_1 : Float(1, 6, 15, strides=[90, 15, 1], requires_grad=0, device=cpu),\n",
      "      %conv_1.weight : Float(30, 6, 5, strides=[30, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv_1.bias : Float(30, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_2.weight : Float(10, 30, 5, strides=[150, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv_2.bias : Float(10, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_3.weight : Float(5, 10, 3, strides=[30, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv_3.bias : Float(5, strides=[1], requires_grad=1, device=cpu),\n",
      "      %linear.weight : Float(5, 25, strides=[25, 1], requires_grad=1, device=cpu),\n",
      "      %linear.bias : Float(5, strides=[1], requires_grad=1, device=cpu),\n",
      "      %label.weight : Float(2, 5, strides=[5, 1], requires_grad=1, device=cpu),\n",
      "      %label.bias : Float(2, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %input : Float(1, 30, 11, strides=[330, 11, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[1], onnx_name=\"Conv_0\"](%actual_input_1, %conv_1.weight, %conv_1.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %input.4 : Float(1, 30, 11, strides=[330, 11, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_1\"](%input) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.8 : Float(1, 10, 7, strides=[70, 7, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[1], onnx_name=\"Conv_2\"](%input.4, %conv_2.weight, %conv_2.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %input.12 : Float(1, 10, 7, strides=[70, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_3\"](%input.8) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.16 : Float(1, 5, 5, strides=[25, 5, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[3], pads=[0, 0], strides=[1], onnx_name=\"Conv_4\"](%input.12, %conv_3.weight, %conv_3.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %onnx::Flatten_16 : Float(1, 5, 5, strides=[25, 5, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_5\"](%input.16) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %onnx::Gemm_17 : Float(1, 25, strides=[25, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"Flatten_6\"](%onnx::Flatten_16) # /tmp/ipykernel_81397/1355192450.py:99:0\n",
      "  %input.20 : Float(1, 5, strides=[5, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_7\"](%onnx::Gemm_17, %linear.weight, %linear.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %onnx::Gemm_19 : Float(1, 5, strides=[5, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_8\"](%input.20) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.24 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_9\"](%onnx::Gemm_19, %label.weight, %label.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %output1 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Softmax[axis=1, onnx_name=\"Softmax_10\"](%input.24) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1834:0\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_sup_save_model = \"paper_v1/models/m6A_3_2_semi_supervised_cnn.best.torch.pickle\"\n",
    "\n",
    "input_example = (1, 6, 15)\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the supervised model for transfer learning\n",
    "model = M6ANet()\n",
    "with open(best_sup_save_model, \"rb\") as fp:\n",
    "    model.load_state_dict(pickle.load(fp))\n",
    "        \n",
    "model = model.to(device)\n",
    "\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "example = torch.rand(input_example).float().to(device)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    example,\n",
    "    f\"{best_sup_save_model}.onnx\",\n",
    "    verbose=True, \n",
    "    input_names=input_names, \n",
    "    output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed2bd43-811d-4e79-a104-afd055f435d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%actual_input_1 : Float(1, 6, 15, strides=[90, 15, 1], requires_grad=0, device=cpu),\n",
      "      %conv_1.weight : Float(30, 6, 5, strides=[30, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv_1.bias : Float(30, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_2.weight : Float(10, 30, 5, strides=[150, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv_2.bias : Float(10, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_3.weight : Float(5, 10, 3, strides=[30, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv_3.bias : Float(5, strides=[1], requires_grad=1, device=cpu),\n",
      "      %linear.weight : Float(5, 25, strides=[25, 1], requires_grad=1, device=cpu),\n",
      "      %linear.bias : Float(5, strides=[1], requires_grad=1, device=cpu),\n",
      "      %label.weight : Float(2, 5, strides=[5, 1], requires_grad=1, device=cpu),\n",
      "      %label.bias : Float(2, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %input : Float(1, 30, 11, strides=[330, 11, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[1], onnx_name=\"Conv_0\"](%actual_input_1, %conv_1.weight, %conv_1.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %input.4 : Float(1, 30, 11, strides=[330, 11, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_1\"](%input) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.8 : Float(1, 10, 7, strides=[70, 7, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[1], onnx_name=\"Conv_2\"](%input.4, %conv_2.weight, %conv_2.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %input.12 : Float(1, 10, 7, strides=[70, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_3\"](%input.8) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.16 : Float(1, 5, 5, strides=[25, 5, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[3], pads=[0, 0], strides=[1], onnx_name=\"Conv_4\"](%input.12, %conv_3.weight, %conv_3.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %onnx::Flatten_16 : Float(1, 5, 5, strides=[25, 5, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_5\"](%input.16) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %onnx::Gemm_17 : Float(1, 25, strides=[25, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"Flatten_6\"](%onnx::Flatten_16) # /tmp/ipykernel_81397/1355192450.py:99:0\n",
      "  %input.20 : Float(1, 5, strides=[5, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_7\"](%onnx::Gemm_17, %linear.weight, %linear.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %onnx::Gemm_19 : Float(1, 5, strides=[5, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_8\"](%input.20) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.24 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_9\"](%onnx::Gemm_19, %label.weight, %label.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %output1 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Softmax[axis=1, onnx_name=\"Softmax_10\"](%input.24) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1834:0\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_sup_save_model = \"paper_v1/models/m6A_revio_s3_noinit_semi_supervised_cnn.best.torch.pickle\"\n",
    "\n",
    "input_example = (1, 6, 15)\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the supervised model for transfer learning\n",
    "model = M6ANet()\n",
    "with open(best_sup_save_model, \"rb\") as fp:\n",
    "    model.load_state_dict(pickle.load(fp))\n",
    "        \n",
    "model = model.to(device)\n",
    "\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "example = torch.rand(input_example).float().to(device)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    example,\n",
    "    f\"{best_sup_save_model}.onnx\",\n",
    "    verbose=True, \n",
    "    input_names=input_names, \n",
    "    output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "863dfc3d-b69d-4f2c-979c-6f91034a22b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%actual_input_1 : Float(1, 6, 15, strides=[90, 15, 1], requires_grad=0, device=cpu),\n",
      "      %conv_1.weight : Float(30, 6, 5, strides=[30, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv_1.bias : Float(30, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_2.weight : Float(10, 30, 5, strides=[150, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv_2.bias : Float(10, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_3.weight : Float(5, 10, 3, strides=[30, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv_3.bias : Float(5, strides=[1], requires_grad=1, device=cpu),\n",
      "      %linear.weight : Float(5, 25, strides=[25, 1], requires_grad=1, device=cpu),\n",
      "      %linear.bias : Float(5, strides=[1], requires_grad=1, device=cpu),\n",
      "      %label.weight : Float(2, 5, strides=[5, 1], requires_grad=1, device=cpu),\n",
      "      %label.bias : Float(2, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %input : Float(1, 30, 11, strides=[330, 11, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[1], onnx_name=\"Conv_0\"](%actual_input_1, %conv_1.weight, %conv_1.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %input.4 : Float(1, 30, 11, strides=[330, 11, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_1\"](%input) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.8 : Float(1, 10, 7, strides=[70, 7, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[1], onnx_name=\"Conv_2\"](%input.4, %conv_2.weight, %conv_2.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %input.12 : Float(1, 10, 7, strides=[70, 7, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_3\"](%input.8) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.16 : Float(1, 5, 5, strides=[25, 5, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[3], pads=[0, 0], strides=[1], onnx_name=\"Conv_4\"](%input.12, %conv_3.weight, %conv_3.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:303:0\n",
      "  %onnx::Flatten_16 : Float(1, 5, 5, strides=[25, 5, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_5\"](%input.16) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %onnx::Gemm_17 : Float(1, 25, strides=[25, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"Flatten_6\"](%onnx::Flatten_16) # /tmp/ipykernel_81397/1355192450.py:99:0\n",
      "  %input.20 : Float(1, 5, strides=[5, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_7\"](%onnx::Gemm_17, %linear.weight, %linear.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %onnx::Gemm_19 : Float(1, 5, strides=[5, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"Relu_8\"](%input.20) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1457:0\n",
      "  %input.24 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"Gemm_9\"](%onnx::Gemm_19, %label.weight, %label.bias) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %output1 : Float(1, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Softmax[axis=1, onnx_name=\"Softmax_10\"](%input.24) # /net/noble/vol1/home/anupamaj/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1834:0\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_sup_save_model = \"paper_v1/models/m6A_2_2_semi_supervised_cnn.best.torch.pickle\"\n",
    "\n",
    "input_example = (1, 6, 15)\n",
    "device = 'cpu'\n",
    "\n",
    "# Load the supervised model for transfer learning\n",
    "model = M6ANet()\n",
    "with open(best_sup_save_model, \"rb\") as fp:\n",
    "    model.load_state_dict(pickle.load(fp))\n",
    "        \n",
    "model = model.to(device)\n",
    "\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "example = torch.rand(input_example).float().to(device)\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    example,\n",
    "    f\"{best_sup_save_model}.onnx\",\n",
    "    verbose=True, \n",
    "    input_names=input_names, \n",
    "    output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ba668-e261-4625-bd68-6d03fdd8d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(f\"{best_sup_save_model}.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb63373-5218-45a5-a0e0-a077433b11e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
